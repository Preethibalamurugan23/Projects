{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load embeddings and labels\n",
    "embeddings_1 = np.load('embeddings_1.npy')\n",
    "embeddings_2 = np.load('embeddings_2.npy')\n",
    "labels_1 = open('icd_codes_1.txt').read().splitlines()\n",
    "labels_2 = open('icd_codes_2.txt').read().splitlines()\n",
    "\n",
    "# Combine embeddings and labels\n",
    "embeddings = np.concatenate([embeddings_1, embeddings_2], axis=0)\n",
    "labels = labels_1 + labels_2\n",
    "\n",
    "# scaler=StandardScaler()\n",
    "# embeddings=scaler.fit_transform(embeddings)\n",
    "\n",
    "# Extract unique ICD10 codes and binarize labels\n",
    "all_labels = [set(l.split(';')) for l in labels]\n",
    "mlb = MultiLabelBinarizer()\n",
    "multi_hot_labels = mlb.fit_transform(all_labels)\n",
    "\n",
    "# Check number of unique codes (should match ~1400)\n",
    "assert multi_hot_labels.shape[1] == len(mlb.classes_)\n",
    "\n",
    "# Split data for training/validation (80-20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(embeddings, multi_hot_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "test_data = np.load('test_data.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Common input shape and output classes\n",
    "input_shape = (1024,)\n",
    "num_classes = len(mlb.classes_)\n",
    "fitted_models = {}\n",
    "\n",
    "# Model 17\n",
    "model17 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='swish', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024, activation='swish', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='swish'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model17.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 18\n",
    "model18 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='selu'),\n",
    "    layers.AlphaDropout(0.2),\n",
    "    layers.Dense(1024, activation='selu'),\n",
    "    layers.AlphaDropout(0.2),\n",
    "    layers.Dense(512, activation='selu'),\n",
    "    layers.AlphaDropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model18.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 19\n",
    "model19 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model19.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 20\n",
    "model20 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model20.compile(optimizer='nadam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 21\n",
    "model21 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model21.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 22\n",
    "model22 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model22.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 23\n",
    "model23 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='swish'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='swish'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model23.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 24\n",
    "model24 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model24.compile(optimizer='nadam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# List of models and their names\n",
    "models_list = [\n",
    "    # (model19, 'model19'),\n",
    "    (model20, 'model20'), (model21, 'model21'), (model22, 'model22'),\n",
    "    (model23, 'model23'), (model24, 'model24')\n",
    "]\n",
    "\n",
    "# Dictionary to store fitted models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop to train and store each fitted model\n",
    "for model, name in models_list:\n",
    "    print(f\"Training {name}\")\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "    fitted_models[name] = model  # Store the fitted model\n",
    "    # Generate predictions on the test data\n",
    "    preds = model.predict(test_data)\n",
    "    # model_name=\"model16\"\n",
    "    pred_labels = (preds >= 0.5).astype(int)\n",
    "\n",
    "    # Decode multi-hot predictions back to ICD10 codes\n",
    "    submission = []\n",
    "    for pred in pred_labels:\n",
    "        codes = [mlb.classes_[j] for j, val in enumerate(pred) if val == 1]\n",
    "        codes.sort()  # Sort lexicographically\n",
    "        label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "        submission.append(label_string)\n",
    "\n",
    "    # Generate sequential IDs (e.g., 1 to number of test samples)\n",
    "    num_test_samples = len(pred_labels)\n",
    "    ids = range(1, num_test_samples + 1)\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_filename = f'submission_{name}.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"Saved {submission_filename}\")\n",
    "    print(f\"Fitted model {name} saved.\")\n",
    "\n",
    "# Access any fitted model using fitted_models['model17'], fitted_models['model18'], etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Common input shape and output classes\n",
    "input_shape = (1024,)\n",
    "num_classes = len(mlb.classes_)\n",
    "\n",
    "# Define 16 models with explicit configurations\n",
    "\n",
    "# Model 25: lr=0.00005, dropout=0.2, dense_size=2048, optimizer=adam\n",
    "model25 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model25.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 26: lr=0.00005, dropout=0.2, dense_size=2048, optimizer=nadam\n",
    "model26 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model26.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 27: lr=0.00005, dropout=0.2, dense_size=1024, optimizer=adam\n",
    "model27 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model27.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 28: lr=0.00005, dropout=0.2, dense_size=1024, optimizer=nadam\n",
    "model28 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model28.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 29: lr=0.00005, dropout=0.3, dense_size=2048, optimizer=adam\n",
    "model29 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model29.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 30: lr=0.00005, dropout=0.3, dense_size=2048, optimizer=nadam\n",
    "model30 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model30.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 31: lr=0.00005, dropout=0.3, dense_size=1024, optimizer=adam\n",
    "model31 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model31.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 32: lr=0.00005, dropout=0.3, dense_size=1024, optimizer=nadam\n",
    "model32 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model32.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 33: lr=0.0001, dropout=0.2, dense_size=2048, optimizer=adam\n",
    "model33 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model33.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 34: lr=0.0001, dropout=0.2, dense_size=2048, optimizer=nadam\n",
    "model34 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model34.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 35: lr=0.0001, dropout=0.2, dense_size=1024, optimizer=adam\n",
    "model35 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model35.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 36: lr=0.0001, dropout=0.2, dense_size=1024, optimizer=nadam\n",
    "model36 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model36.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 37: lr=0.0001, dropout=0.3, dense_size=2048, optimizer=adam\n",
    "model37 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model37.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 38: lr=0.0001, dropout=0.3, dense_size=2048, optimizer=nadam\n",
    "model38 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model38.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 39: lr=0.0001, dropout=0.3, dense_size=1024, optimizer=adam\n",
    "model39 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model39.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 40: lr=0.0001, dropout=0.3, dense_size=1024, optimizer=nadam\n",
    "model40 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model40.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "\n",
    "# List of models and their names\n",
    "models_list = [\n",
    "    # (model25, 'model25'), (model26, 'model26'), (model27, 'model27'),\n",
    "    # (model28, 'model28'), (model29, 'model29'), (model30, 'model30'),\n",
    "    # (model31, 'model31'), (model32, 'model32'), (model33, 'model33'),\n",
    "    # (model34, 'model34'),\n",
    "    (model35, 'model35'), (model36, 'model36'),\n",
    "    (model37, 'model37'), (model38, 'model38'), (model39, 'model39'),\n",
    "    (model40, 'model40')\n",
    "]\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop to train and store each fitted model\n",
    "for model, name in models_list:\n",
    "    print(f\"Training {name}\")\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "    fitted_models[name] = model  # Store the fitted model\n",
    "    # Generate predictions on the test data\n",
    "    preds = model.predict(test_data)\n",
    "    # model_name=\"model16\"\n",
    "    pred_labels = (preds >= 0.5).astype(int)\n",
    "\n",
    "    # Decode multi-hot predictions back to ICD10 codes\n",
    "    submission = []\n",
    "    for pred in pred_labels:\n",
    "        codes = [mlb.classes_[j] for j, val in enumerate(pred) if val == 1]\n",
    "        codes.sort()  # Sort lexicographically\n",
    "        label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "        submission.append(label_string)\n",
    "\n",
    "    # Generate sequential IDs (e.g., 1 to number of test samples)\n",
    "    num_test_samples = len(pred_labels)\n",
    "    ids = range(1, num_test_samples + 1)\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_filename = f'submission_{name}.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"Saved {submission_filename}\")\n",
    "    print(f\"Fitted model {name} saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fitted_models['model37']\n",
    "preds = model.predict(test_data)\n",
    "name=\"model37\"\n",
    "# model_name=\"model16\"\n",
    "pred_labels = (preds >= 0.6).astype(int)\n",
    "\n",
    "# Decode multi-hot predictions back to ICD10 codes\n",
    "submission = []\n",
    "for pred in pred_labels:\n",
    "    codes = [mlb.classes_[j] for j, val in enumerate(pred) if val == 1]\n",
    "    codes.sort()  # Sort lexicographically\n",
    "    label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "    submission.append(label_string)\n",
    "\n",
    "# Generate sequential IDs (e.g., 1 to number of test samples)\n",
    "num_test_samples = len(pred_labels)\n",
    "ids = range(1, num_test_samples + 1)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission2_{name}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"Saved {submission_filename}\")\n",
    "print(f\"Fitted model {name} saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Define a common input shape\n",
    "input_shape = (1024,)\n",
    "num_classes = len(mlb.classes_)\n",
    "\n",
    "# Model 1: Baseline Dense Network with Dropout and L2 Regularization\n",
    "model1 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 2: Deep Feedforward Network with Skip Connections\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Dense(512, activation='relu')(input_layer)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "skip = layers.Concatenate()([input_layer, x])\n",
    "x = layers.Dense(128, activation='relu')(skip)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "output_layer = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "model2 = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 3: Wide and Deep Network\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "wide = layers.Dense(256, activation='relu')(input_layer)\n",
    "deep = layers.Dense(512, activation='relu')(input_layer)\n",
    "deep = layers.Dropout(0.3)(deep)\n",
    "deep = layers.Dense(256, activation='relu')(deep)\n",
    "combined = layers.Concatenate()([wide, deep])\n",
    "x = layers.Dense(128, activation='relu')(combined)\n",
    "output_layer = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "model3 = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model3.compile(optimizer='adam', loss='focal_loss', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 4: LSTM-based Network\n",
    "model4 = models.Sequential([\n",
    "    layers.Reshape((32, 32), input_shape=input_shape),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model4.compile(optimizer='nadam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 5: Transformer-based Model\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Reshape((32, 32))(input_layer)\n",
    "transformer_block = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "x = layers.GlobalAveragePooling1D()(transformer_block)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "output_layer = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "model5 = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model5.compile(optimizer='adamw', loss='binary_focal_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 6: 1D CNN-based Network\n",
    "model6 = models.Sequential([\n",
    "    layers.Reshape((32, 32), input_shape=input_shape),\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model6.compile(optimizer='sgd', loss='hinge', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 7: Shallow Network with RMSProp Optimizer\n",
    "model7 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model7.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.keras.metrics.PrecisionAtRecall(0.8, name=\"PrecisionAtRecall\")])\n",
    "\n",
    "# Model 8: Deep Neural Network with Learning Rate Decay\n",
    "model8 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    # layers.Dense(128, activation='relu'),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model8.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# layers were initially 3 relu, metric doesn't matter, learning rate, decay, preprocess labels\n",
    "\n",
    "# Model 9: Ensemble (Voting Classifier using Averaged Predictions)\n",
    "def ensemble_predict(X):\n",
    "    preds1 = model1.predict(X)\n",
    "    preds2 = model2.predict(X)\n",
    "    preds3 = model3.predict(X)\n",
    "    preds4 = model4.predict(X)\n",
    "    preds5 = model5.predict(X)\n",
    "    preds6 = model6.predict(X)\n",
    "    preds7 = model7.predict(X)\n",
    "    preds8 = model8.predict(X)\n",
    "    return (preds1 + preds2 + preds3 + preds4 + preds5 + preds6 + preds7 + preds8) / 8\n",
    "\n",
    "# List of models\n",
    "models_list = [model1, model2, model3, model4, model5, model6, model7, model8]\n",
    "model_names = [\"model1\", \"model2\", \"model3\", \"model4\", \"model5\", \"model6\", \"model7\", \"model8\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Common input shape and output classes\n",
    "input_shape = (1024,)\n",
    "num_classes = len(mlb.classes_)\n",
    "\n",
    "# Model 9: Deeper Network with Increased Dropout\n",
    "model9 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model9.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# # Model 10: Residual Connections for Better Gradient Flow\n",
    "# input_layer = layers.Input(shape=input_shape)\n",
    "# x = layers.Dense(1024, activation='relu')(input_layer)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "# residual = layers.Dense(512, activation='relu')(x)\n",
    "# x = layers.Add()([x, residual])\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "# output_layer = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "# model10 = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "# model10.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 10: Residual Connections with Shape Matching\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = layers.Dense(1024, activation='relu')(input_layer)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Residual branch\n",
    "residual = layers.Dense(1024, activation='relu')(x)  # Match shape to 1024\n",
    "\n",
    "# Add residual connection\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output_layer = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "model10 = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model10.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "\n",
    "# Model 11: Using LeakyReLU for Better Handling of Negative Activations\n",
    "model11 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model11.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 12: Learning Rate Scheduler with Adam Optimizer\n",
    "model12 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "model12.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 13: Weight Regularization (L1 and L2 Regularization)\n",
    "model13 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model13.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 14: Swish Activation Function for Smoother Gradient Flow\n",
    "model14 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='swish'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='swish'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='swish'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model14.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 15: Using Nadam Optimizer with Batch Normalization\n",
    "model15 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model15.compile(optimizer='nadam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Model 16: \n",
    "model16 = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "model16.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# List of new models\n",
    "new_models = [model9, model10, model11, model12, model13, model14, model15, model16]\n",
    "\n",
    "# # Training loop for new models\n",
    "# for i, model in enumerate(new_models, 9):\n",
    "#     print(f\"Training Model {i}\")\n",
    "#     history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load('test_data.npy')\n",
    "\n",
    "# Generate predictions\n",
    "threshold = 0.5  # Adjust threshold if needed based on validation performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "models_list = [model8]\n",
    "new_models = [model9, model10, model11, model12, model13, model14, model15, model16]\n",
    "model_names = [\"model9\", \"model10\", \"model11\", \"model12\", \"model13\", \"model14\", \"model15\", \"model16\"]\n",
    "# Training loop and submission generation\n",
    "for i, (model, model_name) in enumerate(zip(models_list, model_names), 1):\n",
    "    print(f\"Training {model_name}\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "\n",
    "    # Generate predictions on the test data\n",
    "    preds = model.predict(test_data)\n",
    "    pred_labels = (preds >= 0.3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "models_list = [model16]\n",
    "model_names = [\"model16\"]\n",
    "# models_list = [model13, model14, model15, model16]\n",
    "# model_names = [\"model13\", \"model14\", \"model15\", \"model16\"]\n",
    "# Training loop and submission generation\n",
    "# for i, (model, model_name) in enumerate(zip(models_list, model_names), 1):\n",
    "#     print(f\"Training {model_name}\")\n",
    "\n",
    "model=model16    \n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "\n",
    "# Generate predictions on the test data\n",
    "preds = model.predict(test_data)\n",
    "# model_name=\"model16\"\n",
    "pred_labels = (preds >= 0.4).astype(int)\n",
    "\n",
    "# 0.45 --- 0.438\n",
    "# 0.475 --- 0.440\n",
    "# 0.49 --- 0.441\n",
    "# 0.495 --- 0.442\n",
    "#  0.5 --- 0.442\n",
    "#  0.55 --- 0.442\n",
    "#  0.57 --- 0.442\n",
    "# 0.6 --- 0.441\n",
    "\n",
    "# Decode multi-hot predictions back to ICD10 codes\n",
    "submission = []\n",
    "for pred in pred_labels:\n",
    "    codes = [mlb.classes_[j] for j, val in enumerate(pred) if val == 1]\n",
    "    codes.sort()  # Sort lexicographically\n",
    "    label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "    submission.append(label_string)\n",
    "\n",
    "# Generate sequential IDs (e.g., 1 to number of test samples)\n",
    "num_test_samples = len(pred_labels)\n",
    "ids = range(1, num_test_samples + 1)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission2_{model_name}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"Saved {submission_filename}\")\n",
    "\n",
    "# Ensemble prediction example\n",
    "# ensemble_predictions = ensemble_predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"model16\"\n",
    "# Generate predictions on the test data\n",
    "preds = model.predict(test_data)\n",
    "# model_name=\"model16\"\n",
    "pred_labels = (preds >= 0.44).astype(int)\n",
    "\n",
    "# 0.45 --- 0.438\n",
    "# 0.475 --- 0.440\n",
    "# 0.49 --- 0.441\n",
    "# 0.495 --- 0.442\n",
    "#  0.5 --- 0.442\n",
    "#  0.55 --- 0.442\n",
    "#  0.57 --- 0.442\n",
    "# 0.6 --- 0.441\n",
    "\n",
    "# for new model16, 0.45 - 0.47 yielded 0.460\n",
    "\n",
    "# Decode multi-hot predictions back to ICD10 codes\n",
    "submission = []\n",
    "for pred in pred_labels:\n",
    "    codes = [mlb.classes_[j] for j, val in enumerate(pred) if val == 1]\n",
    "    codes.sort()  # Sort lexicographically\n",
    "    label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "    submission.append(label_string)\n",
    "\n",
    "# Generate sequential IDs (e.g., 1 to number of test samples)\n",
    "num_test_samples = len(pred_labels)\n",
    "ids = range(1, num_test_samples + 1)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = f'submission2_{model_name}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"Saved {submission_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X):\n",
    "    # Get predictions from all models\n",
    "    preds1 = model1.predict(X)\n",
    "    preds2 = model2.predict(X)\n",
    "    preds3 = model3.predict(X)\n",
    "    preds4 = model4.predict(X)\n",
    "    preds5 = model5.predict(X)\n",
    "    preds6 = model6.predict(X)\n",
    "    preds7 = model7.predict(X)\n",
    "    preds8 = model8.predict(X)\n",
    "\n",
    "    # Initialize an empty list to store final predictions\n",
    "    final_predictions = []\n",
    "\n",
    "    # Iterate through each sample's predictions and take the union of labels\n",
    "    for i in range(len(preds1)):\n",
    "        # Collect predictions for the current sample across all models\n",
    "        sample_preds = set(preds1[i]) | set(preds2[i]) | set(preds3[i]) | set(preds4[i]) | \\\n",
    "                       set(preds5[i]) | set(preds6[i]) | set(preds7[i]) | set(preds8[i])\n",
    "\n",
    "        # Convert the set back to a list and add to final predictions\n",
    "        final_predictions.append(list(sample_preds))\n",
    "\n",
    "    return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data\n",
    "preds = model.predict(test_data)\n",
    "pred_labels = (preds >= threshold).astype(int)\n",
    "\n",
    "# Decode multi-hot predictions back to ICD10 codes\n",
    "submission = []\n",
    "for pred in pred_labels:\n",
    "    codes = [mlb.classes_[j] for j, val in enumerate(pred) if val == 1]\n",
    "    codes.sort()  # Sort lexicographically\n",
    "    label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "    submission.append(label_string)\n",
    "\n",
    "# Generate sequential IDs (e.g., 1 to number of test samples)\n",
    "num_test_samples = len(pred_labels)\n",
    "ids = range(1, num_test_samples + 1)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "# Save the submission file\n",
    "submission_filename = 'submission_ensemble1.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"Saved {submission_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of submission files\n",
    "submission_files = [\n",
    "    'ensemble_submission3.csv',\n",
    "    'submission2_model8.csv'\n",
    "]\n",
    "\n",
    "# Read all submissions into a list of DataFrames\n",
    "submissions = [pd.read_csv(file) for file in submission_files]\n",
    "\n",
    "# Initialize an empty DataFrame for the ensemble predictions\n",
    "ensemble_df = pd.DataFrame()\n",
    "ensemble_df['id'] = submissions[0]['id']\n",
    "ensemble_predictions = []\n",
    "\n",
    "# Iterate through each row by index\n",
    "for i in range(len(submissions[0])):\n",
    "    # Initialize a set to store the union of predictions for this row\n",
    "    combined_predictions = set()\n",
    "\n",
    "    # Iterate through each model's prediction for the current sample\n",
    "    for submission in submissions:\n",
    "        # Get the predicted labels for the current sample, handling NaN values\n",
    "        labels_str = str(submission.loc[i, 'labels']).strip()\n",
    "        if labels_str:  # Check if it's not an empty string\n",
    "            labels = labels_str.split(';')\n",
    "            # Add labels to the combined set\n",
    "            combined_predictions.update(labels)\n",
    "\n",
    "    # Convert the set back to a sorted list and join using semicolons\n",
    "    ensemble_prediction = ';'.join(sorted(combined_predictions))\n",
    "    ensemble_predictions.append(ensemble_prediction)\n",
    "\n",
    "# Store the ensemble predictions in the DataFrame\n",
    "ensemble_df['labels'] = ensemble_predictions\n",
    "\n",
    "# Save the ensemble predictions to a CSV file\n",
    "ensemble_df.to_csv('ensemble_submission4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# List of submission files\n",
    "# submission_files = [\n",
    "#     'submission_model1.csv',\n",
    "#     'submission_model2.csv',\n",
    "#     'submission_model8.csv',\n",
    "#     'submission_model4.csv',\n",
    "#     'submission_model5.csv',\n",
    "#     'submission_model6.csv',\n",
    "#     'submission_model7.csv',\n",
    "#     'submission_model8.csv'\n",
    "# ]\n",
    "\n",
    "submission_files = [\n",
    "    'submission1_model8.csv',\n",
    "    'submission2_model8.csv'\n",
    "]\n",
    "\n",
    "# Read all submissions into a list of DataFrames\n",
    "submissions = [pd.read_csv(file) for file in submission_files]\n",
    "\n",
    "# Initialize an empty DataFrame for the ensemble predictions\n",
    "ensemble_df = pd.DataFrame()\n",
    "ensemble_df['id'] = submissions[0]['id']\n",
    "ensemble_predictions = []\n",
    "\n",
    "# Iterate through each row by index\n",
    "for i in range(len(submissions[0])):\n",
    "    # Initialize a Counter to track the frequency of each label\n",
    "    label_counter = Counter()\n",
    "\n",
    "    # Iterate through each model's prediction for the current sample\n",
    "    for submission in submissions:\n",
    "        # Get the predicted labels for the current sample, handling NaN values\n",
    "        labels_str = str(submission.loc[i, 'labels']).strip()\n",
    "        if labels_str:  # Check if it's not an empty string\n",
    "            labels = labels_str.split(';')\n",
    "            # Update the counter with the labels from this prediction\n",
    "            label_counter.update(labels)\n",
    "\n",
    "    # Select only labels that appear more than twice (count > 2)\n",
    "    selected_labels = [label for label, count in label_counter.items() if count > 2]\n",
    "\n",
    "    # Convert the list back to a sorted string of selected labels\n",
    "    ensemble_prediction = ';'.join(sorted(selected_labels))\n",
    "    ensemble_predictions.append(ensemble_prediction)\n",
    "\n",
    "# Store the ensemble predictions in the DataFrame\n",
    "ensemble_df['labels'] = ensemble_predictions\n",
    "\n",
    "# Save the ensemble predictions to a CSV file\n",
    "ensemble_df.to_csv('ensemble_submission3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
