{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1024,)),  # Input layer for 1024-dimensional embeddings\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(mlb.classes_), activation='sigmoid')  # Output layer for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model with binary cross-entropy loss for multi-label classification\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name=\"AUC\", multi_label=True)])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(1024,)),  # Input layer for 1024-dimensional embeddings\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # First dense layer with L2 regularization\n",
    "    layers.Dropout(0.3),  # Dropout layer to prevent overfitting\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Second dense layer with L2 regularization\n",
    "    layers.Dropout(0.3),  # Dropout layer\n",
    "    layers.Dense(1400, activation='sigmoid')  # Output layer for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=128, \n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load('test_data.npy')\n",
    "\n",
    "# Generate predictions\n",
    "preds = model.predict(test_data)\n",
    "threshold = 0.5  # Adjust threshold if needed based on validation performance\n",
    "pred_labels = (preds >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode multi-hot predictions back to ICD10 codes\n",
    "submission = []\n",
    "for pred in pred_labels:\n",
    "    codes = [mlb.classes_[i] for i, val in enumerate(pred) if val == 1]\n",
    "    codes.sort()  # Sort lexicographically\n",
    "    label_string = ';'.join(codes).upper()  # Uppercase and format as required\n",
    "    submission.append(label_string)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Generate sequential IDs (e.g., 0 to number of test samples - 1)\n",
    "num_test_samples = len(pred_labels)  # Length of the test predictions\n",
    "ids = range(1, num_test_samples + 1)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': ids, 'labels': submission})\n",
    "\n",
    "# Save the clean submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Custom F2 loss function\n",
    "def f2_loss(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred, axis=0)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred), axis=0)\n",
    "    f2 = (5 * tp) / (5 * tp + 4 * fn + fp + 1e-8)\n",
    "    return 1 - tf.reduce_mean(f2)  # 1 - F2 to minimize loss\n",
    "\n",
    "# Model architecture with increased complexity\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(1024,)),\n",
    "        layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(1400, activation='sigmoid')  # Multi-label output\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=f2_loss, metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load data\n",
    "embeddings_1 = np.load('embeddings_1.npy')\n",
    "embeddings_2 = np.load('embeddings_2.npy')\n",
    "labels_1 = pd.read_csv('icd_codes_1.txt', header=None)\n",
    "labels_2 = pd.read_csv('icd_codes_2.txt', header=None)\n",
    "test_embeddings = np.load('test_data.npy')\n",
    "\n",
    "# Combine embeddings and labels for training\n",
    "X_train = np.vstack([embeddings_1, embeddings_2])\n",
    "y_train = pd.concat([labels_1, labels_2], ignore_index=True)\n",
    "\n",
    "# Convert labels to multi-hot encoding\n",
    "unique_labels = sorted(set(\";\".join(y_train[0].values).split(\";\")))\n",
    "label_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "def labels_to_multi_hot(labels, label_index):\n",
    "    multi_hot = np.zeros((len(labels), len(label_index)), dtype=int)\n",
    "    for i, label_str in enumerate(labels):\n",
    "        for label in label_str.split(\";\"):\n",
    "            if label in label_index:\n",
    "                multi_hot[i, label_index[label]] = 1\n",
    "    return multi_hot\n",
    "\n",
    "y_train_multi_hot = labels_to_multi_hot(y_train[0], label_index)\n",
    "\n",
    "# Split data for validation (e.g., 80-20 split)\n",
    "split_idx = int(0.8 * len(X_train))\n",
    "X_val, y_val = X_train[split_idx:], y_train_multi_hot[split_idx:]\n",
    "X_train, y_train = X_train[:split_idx], y_train_multi_hot[:split_idx]\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model()\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "\n",
    "# Select a random sample from the validation set\n",
    "sample_size = 1000  # Adjust based on memory capacity\n",
    "sample_indices = random.sample(range(len(X_val)), sample_size)\n",
    "X_val_sample = X_val[sample_indices]\n",
    "y_val_sample = y_val[sample_indices]\n",
    "\n",
    "# Predict on the sample\n",
    "val_preds_sample = model.predict(X_val_sample)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Penalize higher thresholds and limit threshold search range\n",
    "thresholds = np.arange(0.1, 0.5, 0.05)  # Restrict to lower values\n",
    "best_thresholds = []\n",
    "\n",
    "for i in range(y_val_sample.shape[1]):\n",
    "    f2_scores = []\n",
    "    for thresh in thresholds:\n",
    "        preds = (val_preds_sample[:, i] > thresh).astype(int)\n",
    "        \n",
    "        # Calculate micro-F2 score\n",
    "        f2 = fbeta_score(y_val_sample[:, i], preds, beta=2, average='micro')\n",
    "        \n",
    "        # Apply a penalty for higher thresholds (example penalty: subtract a factor based on threshold)\n",
    "        penalty = 0.01 * (thresh - 0.3) if thresh > 0.3 else 0\n",
    "        f2_scores.append(f2 - penalty)\n",
    "        \n",
    "    best_thresh = thresholds[np.argmax(f2_scores)]\n",
    "    best_thresholds.append(best_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test data using a fixed threshold of 0.5\n",
    "test_preds = model.predict(test_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define function to maximize F2 score\n",
    "def f2_threshold_objective(thresh_values):\n",
    "    # Convert array of threshold values to predictions\n",
    "    preds = (val_preds_sample > np.array(thresh_values)).astype(int)\n",
    "    micro_f2 = fbeta_score(y_val_sample, preds, beta=2, average='micro')\n",
    "    return -micro_f2  # Negative because we are minimizing in Bayesian Optimization\n",
    "\n",
    "# Define search space for thresholds (0.1 to 0.5 for each label)\n",
    "space = [Real(0.1, 0.5, name=f'thresh_{i}') for i in range(y_val_sample.shape[1])]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "opt_result = gp_minimize(f2_threshold_objective, space, n_calls=20, random_state=0)\n",
    "\n",
    "# Extract best thresholds\n",
    "best_thresholds = opt_result.x\n",
    "print(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = []\n",
    "# for i in range(test_preds.shape[0]):\n",
    "#     labels = [unique_labels[j] for j in range(test_preds.shape[1]) if test_preds[i, j] > best_thresholds[j]])\n",
    "#     test_labels.append(\";\".join(sorted(labels)))\n",
    "\n",
    "# Predictions on test data using a fixed threshold of 0.5\n",
    "test_preds = model.predict(test_embeddings)\n",
    "test_labels = []\n",
    "for i in range(test_preds.shape[0]):\n",
    "    labels = [unique_labels[j] for j in range(test_preds.shape[1]) if test_preds[i, j] > 0.49]\n",
    "    test_labels.append(\";\".join(sorted(labels)))\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({'id': range(1, len(test_labels) + 1), 'labels': test_labels})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
