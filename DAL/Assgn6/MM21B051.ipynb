{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA5400 Assignment 6\n",
    "## MM21B051 - Preethi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from unidecode import unidecode  \n",
    "\n",
    "# Load the MASSIVE dataset\n",
    "dataset = load_dataset(\"qanastek/MASSIVE\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "We download the data set and split into different files pertaining to each partition: test, train, validation. We then split the dataset into 27 files each for a locale in the list target_locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the required fields from the streaming dataset\n",
    "def extract_relevant_columns(data_split):\n",
    "    for row in data_split:\n",
    "        yield {key: row[key] for key in ['locale', 'partition', 'utt', 'tokens']}\n",
    "\n",
    "# Extract subsets for train, validation, and test\n",
    "train_data = extract_relevant_columns(dataset['train'])\n",
    "test_data = extract_relevant_columns(dataset['test'])\n",
    "validation_data = extract_relevant_columns(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of locales\n",
    "target_locales = [\n",
    "    'af-ZA', 'da-DK', 'de-DE', 'en-US', 'es-ES', 'fr-FR', 'fi-FI', 'hu-HU',\n",
    "    'is-IS', 'it-IT', 'jv-ID', 'lv-LV', 'ms-MY', 'nb-NO', 'nl-NL', 'pl-PL',\n",
    "    'pt-PT', 'ro-RO', 'ru-RU', 'sl-SL', 'sv-SE', 'sq-AL', 'sw-KE', 'tl-PH',\n",
    "    'tr-TR', 'vi-VN', 'cy-GB'\n",
    "]\n",
    "\n",
    "output_dir = \"language_utterances_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete! Utterances saved in language_utterances_2/\n"
     ]
    }
   ],
   "source": [
    "# Function to process and save data from a specific partition\n",
    "def process_and_save_data(data_split, target_locales, partition, deaccent=False):\n",
    "    locale_files = {locale: open(os.path.join(output_dir, f\"{locale}_{partition}.txt\"), \"w\", encoding=\"utf-8\") for locale in target_locales}\n",
    "    \n",
    "    for row in data_split:\n",
    "        locale = row['locale']\n",
    "        if locale in target_locales:\n",
    "            utt = row['utt']\n",
    "            if deaccent:\n",
    "                utt = unidecode(utt)  # Deaccent the utterance if required\n",
    "            locale_files[locale].write(utt + \"\\n\")\n",
    "    \n",
    "    for file in locale_files.values():\n",
    "        file.close()\n",
    "\n",
    "# Process train, test, and validation sets\n",
    "process_and_save_data(dataset['train'], target_locales, partition='train', deaccent=False)\n",
    "process_and_save_data(dataset['test'], target_locales, partition='test', deaccent=False)\n",
    "process_and_save_data(dataset['validation'], target_locales, partition='validation', deaccent=False)\n",
    "\n",
    "print(f\"Data extraction complete! Utterances saved in {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset saved to grouped_data_final\\train.csv.\n",
      "Validation dataset saved to grouped_data_final\\validation.csv.\n",
      "Test dataset saved to grouped_data_final\\test.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def group_partition_to_csv(target_locales, data_dir, partition, output_csv_path):\n",
    "   \n",
    "    combined_data = []\n",
    "    for locale in target_locales:\n",
    "        file_path = os.path.join(data_dir, f\"{locale}_{partition}.txt\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    combined_data.append({\n",
    "                        'text': line.strip(),\n",
    "                        'label': locale\n",
    "                    })\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8') # Save the dataframe to a CSV file\n",
    "    print(f\"{partition.capitalize()} dataset saved to {output_csv_path}.\")\n",
    "\n",
    "def group_all_partitions_to_csv(target_locales, data_dir, output_dir):\n",
    "  \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Group and save for each partition\n",
    "    for partition in ['train', 'validation', 'test']:\n",
    "        output_csv_path = os.path.join(output_dir, f\"{partition}.csv\")\n",
    "        group_partition_to_csv(target_locales, data_dir, partition, output_csv_path)\n",
    "\n",
    "\n",
    "target_locales = ['af-ZA', 'da-DK', 'de-DE', 'en-US', 'es-ES', 'fr-FR', 'fi-FI', 'hu-HU', \n",
    "                  'is-IS', 'it-IT', 'jv-ID', 'lv-LV', 'ms-MY', 'nb-NO', 'nl-NL', \n",
    "                  'pl-PL', 'pt-PT', 'ro-RO', 'ru-RU', 'sl-SL', 'sv-SE', 'sq-AL', \n",
    "                  'sw-KE', 'tl-PH', 'tr-TR', 'vi-VN', 'cy-GB']\n",
    "\n",
    "data_dir = \"language_utterances_2\" \n",
    "output_dir = \"grouped_data_final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Group data into separate CSV files for train, test, and validation\n",
    "group_all_partitions_to_csv(target_locales, data_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_type):\n",
    "    csv_file = os.path.join(output_dir, f'{dataset_type}.csv')\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    texts = data['text']    \n",
    "    labels = data['label']  \n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_data('train')\n",
    "test_texts, test_labels = load_data('test')\n",
    "validation_texts, validation_labels = load_data('validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "We apply the multinomial naive bayes classifier with the best hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline: TF-IDF Vectorizer followed by Multinomial Naive Bayes\n",
    "pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "pipeline.fit(train_texts, train_labels)\n",
    "\n",
    "# Fine-tuning with GridSearchCV on the validation data\n",
    "param_grid = {\n",
    "    'multinomialnb__alpha': [0.5, 1.0, 1.5]  \n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "grid_search.fit(validation_texts, validation_labels)\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Partition Performance:\n",
      "Training Macro average Precision: 0.9780\n",
      "Training Macro average Recall: 0.9762\n",
      "Training Macro average F1 Score: 0.9766\n",
      "\n",
      "Test Partition Performance:\n",
      "Test Macro average Precision: 0.9773\n",
      "Test Macro average Recall: 0.9753\n",
      "Test Macro average F1 Score: 0.9758\n",
      "\n",
      "Validation Partition Performance:\n",
      "Validation Macro average Precision: 0.9920\n",
      "Validation Macro average Recall: 0.9920\n",
      "Validation Macro average F1 Score: 0.9920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on training, test, and validation partitions\n",
    "train_predictions = best_model.predict(train_texts)\n",
    "test_predictions = best_model.predict(test_texts)\n",
    "val_predictions = best_model.predict(validation_texts)\n",
    "\n",
    "# Report metrics for each partition\n",
    "\n",
    "# 1. Training Partition\n",
    "print(\"Training Partition Performance:\")\n",
    "train_report = classification_report(train_labels, train_predictions, zero_division=1, output_dict=True)\n",
    "print(f\"Training Macro average Precision: {train_report['macro avg']['precision']:.4f}\")\n",
    "print(f\"Training Macro average Recall: {train_report['macro avg']['recall']:.4f}\")\n",
    "print(f\"Training Macro average F1 Score: {train_report['macro avg']['f1-score']:.4f}\\n\")\n",
    "\n",
    "# 2. Test Partition\n",
    "print(\"Test Partition Performance:\")\n",
    "test_report = classification_report(test_labels, test_predictions, zero_division=1, output_dict=True)\n",
    "print(f\"Test Macro average Precision: {test_report['macro avg']['precision']:.4f}\")\n",
    "print(f\"Test Macro average Recall: {test_report['macro avg']['recall']:.4f}\")\n",
    "print(f\"Test Macro average F1 Score: {test_report['macro avg']['f1-score']:.4f}\\n\")\n",
    "\n",
    "# 3. Validation Partition\n",
    "print(\"Validation Partition Performance:\")\n",
    "val_report = classification_report(validation_labels, val_predictions, zero_division=1, output_dict=True)\n",
    "print(f\"Validation Macro average Precision: {val_report['macro avg']['precision']:.4f}\")\n",
    "print(f\"Validation Macro average Recall: {val_report['macro avg']['recall']:.4f}\")\n",
    "print(f\"Validation Macro average F1 Score: {val_report['macro avg']['f1-score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test dataset we get a very good precision of 97.7% for the optimised lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "We split the dataset based on continents in the following blocks of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data exported to CSV files successfully.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "locales = ['af-ZA', 'da-DK', 'de-DE', 'en-US', 'es-ES', 'fr-FR', 'fi-FI', 'hu-HU', 'is-IS', 'it-IT',\n",
    "           'jv-ID', 'lv-LV', 'ms-MY', 'nb-NO', 'nl-NL', 'pl-PL', 'pt-PT', 'ro-RO', 'ru-RU', 'sl-SL',\n",
    "           'sv-SE', 'sq-AL', 'sw-KE', 'tl-PH', 'tr-TR', 'vi-VN', 'cy-GB']\n",
    "\n",
    "# Filter and export the train, validation, and test partitions to CSV\n",
    "for partition in ['train', 'validation', 'test']:\n",
    "    partition_data = dataset[partition]\n",
    "    df = pd.DataFrame(partition_data)\n",
    "    df_filtered = df[df['locale'].isin(locales)]\n",
    "    df_filtered = df_filtered[['locale', 'partition', 'utt', 'tokens']]\n",
    "    df_filtered.to_csv(rf\"C:\\Users\\preet\\Downloads\\new_data\\MASSIVE_{partition}.csv\", index=False)\n",
    "    \n",
    "print(\"Filtered data exported to CSV files successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continent files created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the language groups by continent\n",
    "language_groups = {\n",
    "    'Africa': ['af-ZA', 'sw-KE'],\n",
    "    'Asia': ['jv-ID', 'ms-MY', 'tl-PH', 'tr-TR', 'vi-VN'],\n",
    "    'Europe': ['da-DK', 'de-DE', 'es-ES', 'fr-FR', 'fi-FI', 'hu-HU', \n",
    "               'is-IS', 'it-IT', 'lv-LV', 'nb-NO', 'nl-NL', 'pl-PL', 'pt-PT', \n",
    "               'ro-RO', 'ru-RU', 'sl-SL', 'sv-SE', 'sq-AL', 'cy-GB'],\n",
    "    'North America': ['en-US'] \n",
    "}\n",
    "\n",
    "output_dir = r\"C:\\Users\\preet\\Downloads\\Continent_Files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "continent_dataframes = {continent: [] for continent in language_groups.keys()}\n",
    "\n",
    "# Process the files for each partition and append to continent DataFrames\n",
    "for partition in ['train', 'validation', 'test']:\n",
    "    for language in sum(language_groups.values(), []):  \n",
    "        try:\n",
    "            df = pd.read_csv(rf\"C:\\Users\\preet\\Downloads\\new_data\\MASSIVE_{partition}.csv\")\n",
    "            df_language = df[df['locale'] == language]\n",
    "            for continent, languages in language_groups.items():\n",
    "                if language in languages:\n",
    "                    continent_dataframes[continent].append(df_language)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File for {language} not found for partition {partition}. Skipping.\")\n",
    "\n",
    "# Combine and save the continent DataFrames into single CSV files\n",
    "for continent, dfs in continent_dataframes.items():\n",
    "    if dfs:  \n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        combined_df = combined_df[['locale', 'utt', 'tokens']]\n",
    "        combined_df.to_csv(os.path.join(output_dir, f\"{continent}_data.csv\"), index=False)\n",
    "\n",
    "print(\"Continent files created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "# Combine datasets for all continents\n",
    "output_dir = r\"C:\\Users\\preet\\Downloads\\Continent_Files\"\n",
    "continent_files = ['Africa_data.csv', 'Asia_data.csv', 'Europe_data.csv', 'North America_data.csv']\n",
    "df_list = [pd.read_csv(os.path.join(output_dir, file)) for file in continent_files]\n",
    "combined_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# feature and target variable\n",
    "X = combined_data['utt']  # Input features (utterances)\n",
    "y = combined_data['locale']  # Target labels (language locale)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "save_npz('X_train_tfidf.npz', X_train_tfidf)\n",
    "save_npz('X_test_tfidf.npz', X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.sparse import save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "X_train_tfidf = load_npz('X_train_tfidf.npz')\n",
    "X_test_tfidf = load_npz('X_test_tfidf.npz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main issues is that while it's feasible to store and work with the sparse dataset, RDA requires converting it into a dense format, which is extremely memory-inefficient. The dense matrix ends up being over 400GB, making it impossible to use directly. To overcome this, a common and effective strategy is to apply low-frequency pruning by selecting the top 1000 features. Additionally, we reduce the matrix size using Singular Value Decomposition (SVD) by retaining only the top 300 components, which helps compress the data while maintaining critical information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "\n",
    "#Truncated SVD for dimensionality reduction\n",
    "def apply_truncated_svd(X_train, X_test, n_components=300):\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    X_train_reduced = svd.fit_transform(X_train)\n",
    "    X_test_reduced = svd.transform(X_test)\n",
    "    \n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "#defining Regularized Discriminant Analysis(RDA) model\n",
    "class RegularizedDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lambda_param=0.5):\n",
    "        self.lambda_param = lambda_param\n",
    "        self.lda = LinearDiscriminantAnalysis()\n",
    "        self.qda = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.lda.fit(X, y)\n",
    "        self.qda.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        lda_probs = self.lda.predict_proba(X)\n",
    "        qda_probs = self.qda.predict_proba(X)\n",
    "        final_probs = (1 - self.lambda_param) * lda_probs + self.lambda_param * qda_probs\n",
    "        return np.argmax(final_probs, axis=1)\n",
    "\n",
    "#dense matrices\n",
    "X_train_tfidf = load_npz('X_train_tfidf.npz')\n",
    "X_test_tfidf = load_npz('X_test_tfidf.npz')\n",
    "X_train_reduced, X_test_reduced = apply_truncated_svd(X_train_tfidf, X_test_tfidf, n_components=300)\n",
    "\n",
    "#training RDA model\n",
    "rda_model = RegularizedDiscriminantAnalysis(lambda_param=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9525859170085412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       af-ZA       0.93      0.95      0.94      3367\n",
      "       cy-GB       0.97      0.98      0.98      3235\n",
      "       da-DK       0.90      0.80      0.85      3311\n",
      "       de-DE       0.97      0.97      0.97      3358\n",
      "       en-US       0.93      0.97      0.95      3294\n",
      "       es-ES       0.93      0.96      0.94      3297\n",
      "       fi-FI       0.94      0.95      0.95      3375\n",
      "       fr-FR       0.98      0.97      0.98      3220\n",
      "       hu-HU       0.98      0.95      0.97      3330\n",
      "       is-IS       0.96      0.97      0.96      3247\n",
      "       it-IT       0.96      0.96      0.96      3308\n",
      "       jv-ID       0.97      0.92      0.94      3354\n",
      "       lv-LV       0.93      0.97      0.95      3356\n",
      "       ms-MY       0.94      0.97      0.95      3329\n",
      "       nb-NO       0.85      0.87      0.86      3295\n",
      "       nl-NL       0.94      0.92      0.93      3354\n",
      "       pl-PL       0.97      0.96      0.96      3287\n",
      "       pt-PT       0.97      0.94      0.95      3247\n",
      "       ro-RO       0.99      0.95      0.97      3325\n",
      "       ru-RU       0.91      0.99      0.95      3259\n",
      "       sl-SL       0.99      0.95      0.97      3361\n",
      "       sq-AL       0.99      0.98      0.98      3291\n",
      "       sv-SE       0.91      0.96      0.93      3311\n",
      "       sw-KE       0.97      0.99      0.98      3303\n",
      "       tl-PH       0.97      0.98      0.98      3249\n",
      "       tr-TR       0.99      0.96      0.97      3275\n",
      "       vi-VN       0.99      0.99      0.99      3276\n",
      "\n",
      "    accuracy                           0.95     89214\n",
      "   macro avg       0.95      0.95      0.95     89214\n",
      "weighted avg       0.95      0.95      0.95     89214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encodes y_train as numeric\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "# train the RDA model with encoded labels \n",
    "rda_model.fit(X_train_reduced, y_train_encoded)\n",
    "y_pred_encoded = rda_model.predict(X_test_reduced)\n",
    "#convert predictions to strings\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Convert y_test_encoded to strings comparison\n",
    "y_test = label_encoder.inverse_transform(y_test_encoded)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.0, Validation Accuracy: 0.853910477127398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.1111111111111111, Validation Accuracy: 0.857736240913811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.2222222222222222, Validation Accuracy: 0.8624911187626387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.3333333333333333, Validation Accuracy: 0.8730028602138784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.4444444444444444, Validation Accuracy: 0.9163615164598933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.5555555555555556, Validation Accuracy: 0.9421580951339928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.6666666666666666, Validation Accuracy: 0.9420670055200306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.7777777777777777, Validation Accuracy: 0.9420852234428231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.8888888888888888, Validation Accuracy: 0.9420670055200306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1.0, Validation Accuracy: 0.9420487875972382\n",
      "Best lambda: 0.5555555555555556 with validation accuracy: 0.9421580951339928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with lambda 0.5555555555555556: 0.9420035368253257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       af-ZA       0.94      0.94      0.94      2974\n",
      "       cy-GB       0.99      0.97      0.98      2974\n",
      "       da-DK       0.89      0.78      0.83      2974\n",
      "       de-DE       0.98      0.96      0.97      2974\n",
      "       en-US       0.92      0.96      0.94      2974\n",
      "       es-ES       0.94      0.94      0.94      2974\n",
      "       fi-FI       0.99      0.92      0.95      2974\n",
      "       fr-FR       0.99      0.97      0.98      2974\n",
      "       hu-HU       0.98      0.92      0.95      2974\n",
      "       is-IS       0.98      0.95      0.97      2974\n",
      "       it-IT       0.97      0.96      0.96      2974\n",
      "       jv-ID       0.97      0.91      0.94      2974\n",
      "       lv-LV       0.98      0.95      0.96      2974\n",
      "       ms-MY       0.94      0.96      0.95      2974\n",
      "       nb-NO       0.84      0.86      0.85      2974\n",
      "       nl-NL       0.95      0.92      0.93      2974\n",
      "       pl-PL       0.98      0.93      0.96      2974\n",
      "       pt-PT       0.96      0.93      0.95      2974\n",
      "       ro-RO       0.99      0.95      0.97      2974\n",
      "       ru-RU       0.62      1.00      0.77      2974\n",
      "       sl-SL       0.99      0.94      0.96      2974\n",
      "       sq-AL       0.99      0.97      0.98      2974\n",
      "       sv-SE       0.92      0.95      0.93      2974\n",
      "       sw-KE       0.98      0.97      0.98      2974\n",
      "       tl-PH       0.98      0.99      0.98      2974\n",
      "       tr-TR       0.99      0.94      0.96      2974\n",
      "       vi-VN       1.00      0.99      1.00      2974\n",
      "\n",
      "    accuracy                           0.94     80298\n",
      "   macro avg       0.95      0.94      0.94     80298\n",
      "weighted avg       0.95      0.94      0.94     80298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "train_data = pd.read_csv(r'C:\\Users\\preet\\Downloads\\new_data\\MASSIVE_train.csv')\n",
    "validation_data = pd.read_csv(r'C:\\Users\\preet\\Downloads\\new_data\\MASSIVE_validation.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\preet\\Downloads\\new_data\\MASSIVE_test.csv')\n",
    "\n",
    "X_train = train_data['utt']\n",
    "y_train = train_data['locale']\n",
    "\n",
    "X_val = validation_data['utt']\n",
    "y_val = validation_data['locale']\n",
    "\n",
    "X_test = test_data['utt']\n",
    "y_test = test_data['locale']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "n_components = 300  \n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "\n",
    "X_train_reduced = svd.fit_transform(X_train_tfidf)\n",
    "X_val_reduced = svd.transform(X_val_tfidf)\n",
    "X_test_reduced = svd.transform(X_test_tfidf)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  \n",
    "y_val_encoded = label_encoder.transform(y_val)          \n",
    "y_test_encoded = label_encoder.transform(y_test)        \n",
    "\n",
    "class RegularizedDiscriminantAnalysisCV(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lambda_param=0.5):\n",
    "        self.lambda_param = lambda_param\n",
    "        self.lda = LinearDiscriminantAnalysis()\n",
    "        self.qda = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.lda.fit(X, y)\n",
    "        self.qda.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        lda_probs = self.lda.predict_proba(X)\n",
    "        qda_probs = self.qda.predict_proba(X)\n",
    "        final_probs = (1 - self.lambda_param) * lda_probs + self.lambda_param * qda_probs\n",
    "        return np.argmax(final_probs, axis=1)\n",
    "\n",
    "lambda_values = np.linspace(0, 1, 10)  \n",
    "best_lambda = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for lambda_param in lambda_values:\n",
    "    rda_model = RegularizedDiscriminantAnalysisCV(lambda_param=lambda_param)\n",
    "    rda_model.fit(X_train_reduced, y_train_encoded)\n",
    "    y_val_pred_encoded = rda_model.predict(X_val_reduced)\n",
    "    y_val_pred = label_encoder.inverse_transform(y_val_pred_encoded)\n",
    "\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred_encoded)\n",
    "    print(f\"Lambda: {lambda_param}, Validation Accuracy: {accuracy}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_lambda = lambda_param\n",
    "\n",
    "print(f\"Best lambda: {best_lambda} with validation accuracy: {best_accuracy}\")\n",
    "\n",
    "final_rda_model = RegularizedDiscriminantAnalysisCV(lambda_param=best_lambda)\n",
    "final_rda_model.fit(X_train_reduced, y_train_encoded)\n",
    "\n",
    "y_test_pred_encoded = final_rda_model.predict(X_test_reduced)\n",
    "\n",
    "y_test_pred = label_encoder.inverse_transform(y_test_pred_encoded)\n",
    "\n",
    "print(f\"Test Accuracy with lambda {best_lambda}: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RDA, we obtain a test accuracy of 95% on the optimised hyper-parameter lambda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
